{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Goals :\n",
    "1. Implementing a linear Regression Model.\n",
    "2. Defining and testing various loss functions within the Python framework.\n",
    "3. Experimenting with minimizing loss through gradient Descent.\n",
    "4. In the end use the set Model to predict Final grades.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(42)\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data: Using sample set of CS 205 grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>midtermRaw</th>\n",
       "      <th>finalRaw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45.5</td>\n",
       "      <td>62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58.0</td>\n",
       "      <td>60.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>68.0</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>64.5</td>\n",
       "      <td>50.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>74.0</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   midtermRaw  finalRaw\n",
       "3        45.5      62.0\n",
       "4        58.0      60.5\n",
       "5        68.0      32.0\n",
       "6        64.5      50.5\n",
       "7        74.0      51.0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/CS205_grades_12_19_18_Final.csv\")\n",
    "df_cleaned = df[['midtermRaw','finalRaw']]\n",
    "# drop all undefined rows \n",
    "df_cleaned = df_cleaned.dropna() \n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data using the MinMaxScaler. Giving each Midterm and Final score a value between 0 and 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "x = df_cleaned.dropna().iloc[:,[0]]\n",
    "y = df_cleaned.dropna().iloc[:,[1]]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_scaled_values = scaler.fit_transform(x)\n",
    "X_scaled_values[:,:] = X_scaled_values\n",
    "Y_scaled_values = scaler.fit_transform(y)\n",
    "Y_scaled_values[:,:] = Y_scaled_values\n",
    "\n",
    "x = X_scaled_values\n",
    "y = Y_scaled_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Data to see if Linear Regression would be a good fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x233a0c2d0f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAdrklEQVR4nO3dfYxc11nH8e+TjUM3ULJpbaR2bcdGuGlNg+pmSYMs0YYU4kQitpJAYxRoUajFS0C8WXJUVKqAFFMLIpDCiwVReU36ksisiMGI2lVRhEPW2japTYxMaGKvK7LQbv7JQh3z8MfMmvHsvTP3ztyXc+79fSRLu7PXc8+9M/PMOc99zrnm7oiISPyuqLsBIiJSDAV0EZGGUEAXEWkIBXQRkYZQQBcRaYgr69rx2rVrfdOmTXXtXkQkSidOnPhPd1+X9LfaAvqmTZuYm5ura/ciIlEys5fT/qaUi4hIQyigi4g0hAK6iEhDKKCLiDSEArqISEMooIuINIQCuohIQyigi4g0xNCAbmaPmdmrZvaVlL+bmf2emZ0xs+fN7L3FN1NEJE6H5hfYvv8om/c9zfb9Rzk0v1DavrL00D8F7Bjw99uBLd1/e4A/GL9ZIiLxOzS/wINPvcDC0jIOLCwt8+BTL5QW1IcGdHf/IvD1AZvsBP7MO44DU2b2tqIaKCISqwNHTrN84eJljy1fuMiBI6dL2V8Ra7lMA2d7fj/Xfexr/Rua2R46vXg2btxYwK5FpAqH5hc4cOQ055eWefvUJHtvu55d26brblbwzi8t53p8XEVcFLWExxJvVOruB919xt1n1q1LXCxMRAJTddqgSd4+NZnr8XEVEdDPARt6fl8PnC/geUUkAFWnDZpk723XM7lm4rLHJtdMsPe260vZXxEBfRb4iW61y83Aa+6+Kt0iInGqOm3QJLu2TfPwXTcwPTWJAdNTkzx81w2lpauG5tDN7HHgA8BaMzsH/DqwBsDd/xA4DNwBnAFeB36ylJaKSC3ePjXJQkLwLitt0DS7tk1Xdr1haEB3991D/u7AzxXWIhEJyt7brufBp164LO1SZtpARlfbHYtEJA4rvcuQqlxUdZNMAV1EhqoybTDMStXNyohhpeoGCKaNddFaLiISFVXdpFNAF5GoqOomnVIuIlKqovPdqrpJpx66iJQm7yzTLCsTVj1ZJyYK6CJSmjz57qzBv+rJOjFRykVESpMn3z0o+PcH65CqbkKiHrqIlCbP4lS62Dk+BXQRKU2efHfVKxM2kQK6iJQmT75bFzvHpxy6iJQqa767d4mBhaVlJswuu4CqnPlw6qGLSDB2bZu+1FO/6J375OiGGtkpoItIUDS1f3RKuYg0VJEzNKtc3VDVLqNTQBdpoCJXJKx6dUNN7R+dUi4iDVRk2qLqFIiqXUanHrpIAxWZtqg6BaJql9Gphy7SQEVO0qljwo+qXUajgC7SQEWmLepKgajaJT+lXEQaqMj7gNZ1T9E6q11ivWepArpIQxW5ImEdqxvWVe0S8z1LlXIRkSAp1ZOfeugiEqQ2pnrGpYAuIrXIkqduU6qnCAro0jqxXvBKEuuxhJyn3nvb9Ze1DeKZ2KQcurRK3psWhyzmYwk5Tx3zPUvVQ5dWyXPfyjzq6CmXdSxVCD1P3Z/qOTS/wPb9Ry/NXL3oznSAIyIFdGmVMgJJXemD0IPiIDHlqftf3/6Zq1B/mmhFppSLme0ws9NmdsbM9iX8faOZHTOzeTN73szuKL6pIuMrYxp7XemDmO/BGdMCXEmv74pQ0kQrhgZ0M5sAHgVuB7YCu81sa99mvwZ8xt23AfcCv190Q0Xg/4e+m/c9zfb9R3Pni0cNJIP2W1dPOaag2C+mPPWw1zGkEVGWlMtNwBl3fwnAzJ4AdgKnerZx4Nu7P18DnC+ykdJMefPORaQ2RqltHrbfutIHddVpF6WOksRRpL2+vX8PhXk3H5S6gdk9wA53/6nu7z8OvM/dH+jZ5m3A3wPXAt8KfNDdTyQ81x5gD8DGjRtvfPnll4s6DolMf5CETu9yUC9t5aJUv+mpSZ7Z9wOltXXYfkc5FolH0uu7oo7X2cxOuPtM0t+y5NAt4bH+b4HdwKfcfT1wB/DnZrbqud39oLvPuPvMunXrMuxammqUvHNdqY1h+40pfSD59b6+ABPWCYnTU5PcfeM0B46cHjkFWLQsKZdzwIae39ezOqVyP7ADwN3/yczeBKwFXi2ikdI8owTnulIbWfYbS/ogRDFMjkp6fUOcHJWlh/4csMXMNpvZVXQues72bfMKcCuAmb0LeBOwWGRDpVlGqdCo6yJg3v2Oe+G2TTQ5qlhDA7q7vwE8ABwB/oVONctJM3vIzO7sbvYrwEfN7MvA48BHfFhyXlptlOBcV2ojz35jDlB1CDEoZhXiPIBME4vc/TBwuO+xj/f8fArYXmzTpMmGVWikDcPrSm1k3W9MszerTnUk7S/EoJhV1hRgledZM0WlNmlBMsTc5DArH9q08rbQAtSwc1x0EErb3zWTa1havrBq+5BKAdNkWcSr6veyFueS4MQ2DO9Ns6QJLUANOsdlpI3S9mdGoydHVf1eVg9dglPVMHycXmjv/72iu1hTmhAD1KBzXEbaKG1/S69f4JEPvSf4Kpc0w1JxVaeUFNAlOFWUJ44zFE5brClJiCvyweBzXEYQGrS/Jpd8Vl1qq5SLBKeK8sRxhsKDFmvqtTKTNMRgNegcl7HoV8zrzoyj6uNWD12CU8UaJeP0QrNsE3qwGnaO0y72jZqmGvU1jWHS0SBVr7czdC2XsszMzPjc3Fwt+xYZZ12YtP87Ycb/ukcZePolBVJIDvRlzQXQGjnJBq3loh66tNI4941M+7+jBpo8vdCqeqxJee3t+49WWmMfU01/KBTQpZXGGQoXOYzOc3G27vr8qis2Yp50VBcFdJERFFWZkacXWnePteqKjZhuUxcKVblIK4Wy5kqeXmjdPdaqKzbaWhkzDgV0aaVQZqPmKRGs+x6iVS+OpnXm81PKRVqp7t7uijwXZ8e5kAvFXFCtehJQkycdlUEBXVqpzvxsf2C9+8Zpjr24ODTQjnMxtu4LqlIN1aFLK9VV41zXfuu6H6sUT3XoIj1WesjLFy4y0V1YK23NlaLrvuuqVAklxSTlUkCXVklaWGslD52l7nvv577MJ2ZP8tryhZECfF2BVSWA7aCALlEoqqc8bt33hYt+6YYMo+ShhwXWsmaCjntBdUXsa6s0ncoWJXhF1owXUffdK2+p4y3vXJf6eJm18UWUAIZSuy/p1EOX4BWZd86Tekjbtl+edMmxFxdTHz/24mKp+fVxSwDrnqk6rjaMLtRDl9ocml9g+/6jbN73NNv3H03t6RWZd84z+zBp2yR58tCDjiX0C5eht2+QtowuFNClFnk+YEXOkMyTeujf9tqr17DmCrtsm7x56EHHUvdM0GFCb98gocwMLptSLlKLPMP3oi7orciTeujf9tcOvcDjz57lojsTZtx9Y740xrBjKfI4i1b061ClmEcXeSigSy3yfMCqvutLmkPzCzx5YuHSPUQvuvPkiQVmrntLri8IGHwseY6zyrxwKK/DKNpStqmZolKLGGcuhtZm3dEnuyadq0EzRZVDl1rEuDRqaMP2WPLCWS9+l6ktKzcq5SKlGZQOCH34ntT20IbtoX3BJAlpUbA2rNyogC6lyPJBDvUDltb2u2+c5skTC8FcFJy6eg3feP1C4uOhiL12PTZKuUgpYkkHJElr+7EXF1cN2+++cZoDR06Xlk4YlK5Iu/xV02WxRDGMIppEPXQpRdEf5HGqOfL+30Ft7x1VlJlOODS/wCdmT15aNybp+V9bXt07B1Ifr0NamuoKMzbvezq4VFvsMvXQzWyHmZ02szNmti9lmx81s1NmdtLM/qrYZkpsipyEMs4sv1H+b9a2lzUKWWnzUkJg7n3+GCb6pM22veje6BmbdRka0M1sAngUuB3YCuw2s61922wBHgS2u/t3A79YQlslIkVWsYwTOEf5v1nbXlY6IanNSc8fQ6VQf3XJhNmqbWJJxcUgS8rlJuCMu78EYGZPADuBUz3bfBR41N2/AeDurxbdUIlLkVUs4wTOUf5v1raXVfUy7LhWnj/0SqEVvWmqzfueTtymiJx6GxbfGiZLQJ8Gzvb8fg54X9827wAws2eACeAT7v53/U9kZnuAPQAbN24cpb0SkaKqWMYJnKP+3yxtL2sq/KBVHvufP9RKoTRlfQmGVB5Zpyw59NVjJOi/jn4lsAX4ALAb+GMzm1r1n9wPuvuMu8+sW5e8LrRIv3FSC2WmJcqarJKWd7726jXRT4Yp6/WIuaqqSFl66OeADT2/rwfOJ2xz3N0vAP9uZqfpBPjnCmmltNo4qYWy0xJl9JBjSaWMoqxjU3lkx9C1XMzsSuBfgVuBBTpB+sfc/WTPNjuA3e7+YTNbC8wD73H3/0p7Xq3lIk2nnG51Qltnp0xjreXi7m8ADwBHgH8BPuPuJ83sITO7s7vZEeC/zOwUcAzYOyiYizRdW26oEIoYKn6qoNUWpXZN7Mm2qccYiia+j5IM6qFrpqjUKoTqhDICQcg53aYGvtgqfsqggN5iIXyw6168qawvlNBWZlwRwhdoaEL4HBRFi3O1VCg53rp7smWVu4Wa01V53+VC+RwURT30lhrWM07rtfTfU3P3+zbwm7tuGLkddfdky/pCqar0sMiFx9qo7hFi0RTQW2rQBzttWP7ZuVd45t++fmnbi+78xfFXAEYO6nXfeLjML5Syc7qjpE/q/gINTdO+4JRyaalBK/Wl9Vp6g3mvx589m/h4FnXfGizU1EgWZS481hYxrFiZh3roLTWoZ/xLn/5Srue6OGbpa53VCTHPyixz4bEihXzRse4RYtEU0ANX1odh0Af7wJHTqYtDJUlaErVq45ynPF8oIQWnMhceK0roVTUxf6En0cSigPV/GKDTeyg7JZG23/duvCYx7XLfzRvHujA6rqrOU12vRyztSaIJVsUba+q/1KfoErNB96fslZbX/suPfh/33bzxUo98wmxVMM+6jyJVVYqXdT9VnYO6rz9k0bSLjqFTyqUGWYftRX4Y8g5904blv7nrhtTeeF3D66qCRpb95DkHRaRvQp8dqaqaaqmHXrE8ExmKvAJfRS+2rkkrVVUqZNlPnl58kya0pFFVTbUU0Cs26APfP1S/5Z3rCvswVNGLLWMfWdIXVQWNLPvJeg7aMmMzhrRQkyjlUrG0D/xKD613qP7kiQXuvnGaYy8ujn0Fvoqhb9H7yJq+qKpSoXc/C0vLTJhdFoR3bZvOfA7alFsOPS3UJAroFUv7wK8Eh17LFy5y7MXFQqoBqqi3LXofeaZlVxU0VvaR9kWT9RwotyxlUMqlYmnD9rTJOUX12KoY+ha9j1B7scO+aLKcA+WWpQzqoVcsLT2QNpmnyB5bbEPfUHuxw75ospznLOkbqV9IE8myUECvQdoHPvYpyEWXLYY6LbuoL5ph6ZukcxZbgIlZ6LNckyjlEogmVAMUXbkR6jkpMl2S55y1pdQxFDFWIqmHHpCiUyJV9+aGpSJGaU+IaaIiq2ryXCcoa+1u9fqThXoNZxAF9IaqY7g4KBUR4vC1qgW9BsmTvhm2hv0oxxLi6xKKUK/hDKKUS0PVMVwclIoIbfgaSvrilneuy/x4WiCZunrNyMcS2usSkhgrkRTQG6qO4WJazhtIXY63ruFrKIHs2IuLmR9PCzDujHwsMaYVqhLqNZxBlHLJKZZ8Y9nDxbTz0J+KWOkJD2pnHUIJZHnakZa7T7shSZZjiTGtUKUQr+EMEm1AryOwxpRvLLPkL895SOoJF92eYZLeK9dMrmFp+cKqba+ZXFN6e3rlDahJAWacOQyhlobKaKJMudSV/wxlmJ5FmcPFPOdhUC+xiuFr2nvlm28kf8lUffOlIvK04zxHjGkFSRdlD72s8q1hQhmmZ1XEcDGpd5vnPKT1QKenJisJGmnvlTRLr6/utZepiBLIcZ8jtrSCpIsyoNcVWNuWb0xLraSlK5LOQ91D+rzviTpeyyICqoKyQKQpl6puaNAvxjKmcaT1bs3IfB7qHtKnvSeuvXpNq15LaYdMPXQz2wH8LjAB/LG770/Z7h7gs8D3untpd4Cuq9c36tA2lsqYfmm926XXL/DIh96T+Zjq7D2mvVd+/Ye/G2jO3d7HEev7U1YbGtDNbAJ4FPhB4BzwnJnNuvupvu3eDPwC8GwZDe1V1Q0N0vadZz8xVcb0G5RiimWIn/ReueWd6y77/ZEPvSeKYxlEM0UFsvXQbwLOuPtLAGb2BLATONW33W8AnwR+tdAWpogloNR1AbcIdee/i9L7XmliABvnmGJ+f8pqWXLo08DZnt/PdR+7xMy2ARvc/W8GPZGZ7TGzOTObW1xMniHXNLFVxvSqO/9dhphKT7Ma55hifn/Kall66EmVuZdur2NmVwCPAB8Z9kTufhA4CDAzM5N8i56Gib0yJmnm5/b9R6PNt4YQwIrOWY9zTLG/P+VyWXro54ANPb+vB873/P5m4N3AF8zsq8DNwKyZzRTVyJjFUBmzEqQ373ua7fuPpk7QCmVBq3HUVSG1ooxzOM4xxfD+lOyyBPTngC1mttnMrgLuBWZX/ujur7n7Wnff5O6bgOPAnWVWucQk9LRFngDThHRF3QGsjHOomaKyYmjKxd3fMLMHgCN0yhYfc/eTZvYQMOfus4OfIRx1lWeFfAE3z0WxvEP7EMvh6qyQgnJSPpopKisy1aG7+2HgcN9jH0/Z9gPjN6t4TaxuKEIR0/iThvYhn+86A1hZOWsFZYFIZ4qOognpgjLkyb/mGdrrfCerO+UjzRblWi6jCKG6oQp50xx5as3zDO3bcr7zqjvlI83WmoDehvKsUdIceQNM1qF9nec77UstlJx+KOmRUM6HFMfc6ykHn5mZ8bm56gph+oMddHqiTbqiv33/0dSlap/Z9wOVtqWu852237tvnObJEwuNfv3zaMPnoanM7IS7J5aFtyaH3obyrJDSHHWd77Tc/ePPnlVOv4eucTRTa1IuEM5QdxRZhsehpZXqON9pX14XU0aibc3ph/TlL8VpTQ89Zlkn/6iCIv3LayLl3nJNuoaSR90zZqUcCugRyDo8bkNaaZi0L7Xd79vQ+i+7Xvryb6ZWpVxilWd4nJTmaFM1w6CqnZnr3jLwPOg8Nfd426I1VS4xG6d6RdUM2eg8SSxU5RK5cYbHqmbIRudJmkAplwgMGh4PSxOomiEbnSdpAgX0SKTlxofNDA2tlDFUOk/SBEq5RCxLmkDVDNnoPEkTqIcesSxpAlUzZKPzJE2ggB6xrGmCmGfIZlVEyWEbzpM0m1IuEVOaoKMJ9zoVKYICesQ0M7RDJYciHUq5EM4MwVHaoTSBSg5FVrQ+oIdy78tQ2hEjlRyKdLQ+5RLKcD2UdsRI1xJEOlrfQw9luB5KO2KkkkORjtYH9FCG66G0I1a6liCilEsww/VQ2iEi8Wp9Dz2U4Xoo7RCReGk9dBGRiGg9dBGRFlBAFxFpiNbn0EWKFsrMY2kfBXSRAmnGr9QpU8rFzHaY2WkzO2Nm+xL+/stmdsrMnjezz5vZdcU3VSR8mvErdRoa0M1sAngUuB3YCuw2s619m80DM+7+PcDngE8W3VCRGGjGr9QpSw/9JuCMu7/k7t8EngB29m7g7sfc/fXur8eB9cU2UyQOaTN7NeNXqpAloE8DZ3t+P9d9LM39wN8m/cHM9pjZnJnNLS4uZm+lSCQ041fqlOWiqCU8ljgbyczuA2aA9yf93d0PAgehM7EoYxsLo+oDKZtm/EqdsgT0c8CGnt/XA+f7NzKzDwIfA97v7v9TTPOKo+oDqYoWCpO6ZEm5PAdsMbPNZnYVcC8w27uBmW0D/gi4091fLb6Z41P1gYg03dAeuru/YWYPAEeACeAxdz9pZg8Bc+4+CxwAvg34rJkBvOLud5bY7txUfSD9lIKTpsk0scjdDwOH+x77eM/PHyy4XYXTeuMKYL2UgpMmas1aLm2vPlgJYAtLyzj/H8AOzS/U3bRaKAUnTdSagL5r2zQP33UD01OTGDA9NcnDd93Qmt6YAtjllIKTJmrVWi5trj5QALucUnDSRK3pobedZjBeru0pOGkmBfSWUAC7XNtTcNJMrUq5tJlmMK7W5hScNJMCeosogIk0m1IuIiINoYAuItIQCugiIg2hgC4i0hAK6CIiDaGALiLSEAroIiINoYAuItIQCugiIg2hgC4i0hAK6CIiDaGALiLSEAroIiINoYAuItIQCugiIg2hgC4i0hAK6CIiDaGALiLSEAroIiINoYAuItIQCugiIg2hgC4i0hCZArqZ7TCz02Z2xsz2Jfz9W8zs092/P2tmm4pu6CgOzS+wff9RNu97mu37j3JofqHuJomIlGZoQDezCeBR4HZgK7DbzLb2bXY/8A13/y7gEeC3im5oXofmF3jwqRdYWFrGgYWlZR586gUFdRFprCw99JuAM+7+krt/E3gC2Nm3zU7gT7s/fw641cysuGbmd+DIaZYvXLzsseULFzlw5HRNLRIRKVeWgD4NnO35/Vz3scRt3P0N4DXgrf1PZGZ7zGzOzOYWFxdHa3FG55eWcz0uIhK7LAE9qaftI2yDux909xl3n1m3bl2W9o3s7VOTuR4XEYldloB+DtjQ8/t64HzaNmZ2JXAN8PUiGjiqvbddz+Saicsem1wzwd7brq+pRSIi5coS0J8DtpjZZjO7CrgXmO3bZhb4cPfne4Cj7r6qh16lXdumefiuG5iemsSA6alJHr7rBnZt688WiYg0w5XDNnD3N8zsAeAIMAE85u4nzewhYM7dZ4E/Af7czM7Q6ZnfW2ajs9q1bVoBXERaY2hAB3D3w8Dhvsc+3vPzfwM/UmzTREQkD80UFRFpCAV0EZGGUEAXEWkIBXQRkYZQQBcRaQgFdBGRhlBAFxFpCKtrQqeZLQIvF/BUa4H/LOB5YqHjba42HSvoeEd1nbsnLoZVW0AvipnNuftM3e2oio63udp0rKDjLYNSLiIiDaGALiLSEE0I6AfrbkDFdLzN1aZjBR1v4aLPoYuISEcTeugiIoICuohIY0QT0M1sh5mdNrMzZrYv4e/fYmaf7v79WTPbVH0ri5HhWH/ZzE6Z2fNm9nkzu66OdhZl2PH2bHePmbmZRV3qluV4zexHu6/xSTP7q6rbWKQM7+eNZnbMzOa77+k76mhnEczsMTN71cy+kvJ3M7Pf656L583svYU2wN2D/0fnTkn/BnwncBXwZWBr3zY/C/xh9+d7gU/X3e4Sj/UW4Oruzz8T67FmPd7udm8GvggcB2bqbnfJr+8WYB64tvv7d9Td7pKP9yDwM92ftwJfrbvdYxzv9wPvBb6S8vc7gL8FDLgZeLbI/cfSQ78JOOPuL7n7N4EngJ192+wE/rT78+eAW83MKmxjUYYeq7sfc/fXu78ep3Pj7lhleW0BfgP4JPDfVTauBFmO96PAo+7+DQB3f7XiNhYpy/E68O3dn69h9U3oo+HuX6RzG840O4E/847jwJSZva2o/ccS0KeBsz2/n+s+lriNu78BvAa8tZLWFSvLsfa6n843fqyGHq+ZbQM2uPvfVNmwkmR5fd8BvMPMnjGz42a2o7LWFS/L8X4CuM/MztG51eXPV9O0WuT9fOeS6Z6iAUjqaffXW2bZJgaZj8PM7gNmgPeX2qJyDTxeM7sCeAT4SFUNKlmW1/dKOmmXD9AZff2jmb3b3ZdKblsZshzvbuBT7v7bZvZ9dG44/253/9/ym1e5UuNULD30c8CGnt/Xs3pYdmkbM7uSztBt0NAnVFmOFTP7IPAx4E53/5+K2laGYcf7ZuDdwBfM7Kt08o6zEV8Yzfpe/mt3v+Du/w6cphPgY5TleO8HPgPg7v8EvInOQlZNlOnzPapYAvpzwBYz22xmV9G56Dnbt80s8OHuz/cAR717FSIyQ4+1m4L4IzrBPOb8Kgw5Xnd/zd3Xuvsmd99E55rBne4+V09zx5blvXyIzoVvzGwtnRTMS5W2sjhZjvcV4FYAM3sXnYC+WGkrqzML/ES32uVm4DV3/1phz173VeEcV4/vAP6VzhXzj3Ufe4jOhxs6b4LPAmeAfwa+s+42l3is/wD8B/Cl7r/Zuttc5vH2bfsFIq5yyfj6GvA7wCngBeDeuttc8vFuBZ6hUwHzJeCH6m7zGMf6OPA14AKd3vj9wE8DP93z2j7aPRcvFP1e1tR/EZGGiCXlIiIiQyigi4g0hAK6iEhDKKCLiDSEArqISEMooIuINIQCuohIQ/wfLH1iSZwJukkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "plt.scatter(x,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From this we can see that Linear Regression would be a good fit due to the fit of the scatterplot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to create a manual Linear Regression Line\n",
    "## First Define a basic function intended to return the product of theta0,theta1, and x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function h(theta)\n",
    "def h(theta0, theta1, x):\n",
    "    return theta0 + theta1*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually Define a square loss(L2) Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.023489423483239062"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sqerror(x, y, theta0, theta1):\n",
    "    return np.mean((y-h(theta0,theta1,x))*(y-h(theta0,theta1,x)))\n",
    "    \n",
    "sqerror(x, y, 0.29,0.52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the L1 absolute error function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1258878083450344"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def abserror(x, y, theta0, theta1):\n",
    "\n",
    "    output = np.mean(abs(y-h(theta0,theta1,x)))\n",
    "    return output\n",
    "\n",
    "abserror(x, y, 0.29,0.52)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a function called huberror that computes the pseudo huber error based off of the 2 arguments provided:\n",
    "The equation for this function is given by \n",
    "![image.png](attachment:image.png)\n",
    "The following function finds the average huber error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006994227909507368"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def huberror(x, y, theta0, theta1, delta):\n",
    "\n",
    "    a = ((y - h(theta0, theta1,x))/delta)**2\n",
    "    b = np.sqrt(1 + a) - 1\n",
    "    sqard = delta**2\n",
    "    return np.mean(sqard * b)\n",
    "    \n",
    "\n",
    "huberror(x, y, 0.29,0.52,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the spirit of trying to understand the impact higher or lower theta's have on the ouput, I created an slider to show the difference. Try it out for yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: ok\n"
     ]
    }
   ],
   "source": [
    "from ipywidgets import interact\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d68a91e17ad4c2f8b20801e4dba1ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='theta0', max=1.0), FloatSlider(value=0.0, descriptioâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import pylab\n",
    "import numpy\n",
    "\n",
    "def f(theta0, theta1):\n",
    "  \n",
    "\n",
    "    y1 = h(theta0, theta1, x) \n",
    "    pylab.plot(x,y1) \n",
    "    \n",
    "    sqerr = round(sqerror(x, y, theta0, theta1),6)\n",
    "    abserr = round(abserror(x, y, theta0, theta1),4)\n",
    "    huberr = round(huberror(x, y, theta0, theta1, 0.01),4)\n",
    "    pylab.title('L1=' + str(abserr) + '  L2=' + str(sqerr) + '  hub=' + str(huberr))\n",
    "    \n",
    "    pylab.scatter(x, y, alpha=0.5)\n",
    "    pylab.show() \n",
    "\n",
    "interact(f, theta1=(0,1,0.1), theta0=(0,1,0.1));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are the \"best\" values for each error function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# BEST VALUES FOR AVE SQUARE ERROR\n",
    "theta0 = .10\n",
    "theta1 = .80\n",
    "error = .0014\n",
    "# BEST VALUES FOR AVE ABS ERROR\n",
    "theta0 = .10\n",
    "theta1 = 1.00\n",
    "error = .0015\n",
    "\n",
    "# BEST VALUES FOR AVE HUBER ERROR\n",
    "theta0 = .10\n",
    "theta1 = 1.00\n",
    "error = .0015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent Exploration\n",
    "In this task we use the Gradient descent methods to find a \"better\" values for theta0 and theta1 that minimizes the error. Gradient descent is an iterative algorithm. It computes values of theta0 and theta1 in the direction of reaching the minimum point in the error function. The iterative formulas using L2 loss function for theta0 and theta1 are given by:\n",
    "$$\n",
    "\\theta_0 = \\theta_0 - \\alpha*(\\sum(\\theta_1*x_j + \\theta_0)-y_j)\n",
    "$$\n",
    "$$\n",
    "\\theta_1 = \\theta_1 - \\alpha*(\\sum(\\theta_1*x_j + \\theta_0 - y_j)*x_j\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute using Gradient Descent algorithm (L2 loss)\n",
    "\n",
    "Given the observed data (obsX,obsY), learning rate (alpha), and desired error threshold, \n",
    "The function returns theta0 and theta1 when it reaches the error threshold.\n",
    "The convergence is reached when the abs(newError - oldError) is less than the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01139455] [0.0066027] 0.0028462579756407856\n",
      "[0.0169804] [0.00984237] 0.0027788956936028405\n",
      "[0.02249325] [0.01304162] 0.002713262778008837\n",
      "[0.02793403] [0.01620098] 0.0026493148136392565\n",
      "[0.03330368] [0.01932097] 0.0025870085260032453\n",
      "[0.03860313] [0.02240209] 0.002526301752040954\n",
      "[0.04383327] [0.02544484] 0.00246715341157832\n",
      "[0.04899501] [0.02844972] 0.002409523479514994\n",
      "[0.05408922] [0.03141722] 0.0023533729587265737\n",
      "[0.05911679] [0.03434782] 0.0022986638536627935\n",
      "[0.06407857] [0.037242] 0.0022453591446237996\n",
      "[0.06897542] [0.04010022] 0.0021934227626970973\n",
      "[0.07380816] [0.04292295] 0.0021428195653382036\n",
      "[0.07857765] [0.04571065] 0.0020935153125784547\n",
      "[0.08328468] [0.04846378] 0.002045476643843895\n",
      "[0.08793007] [0.05118277] 0.001998671055369519\n",
      "[0.09251462] [0.05386807] 0.0019530668781935951\n",
      "[0.0970391] [0.05652011] 0.0019086332567171693\n",
      "[0.10150431] [0.05913932] 0.001865340127814227\n",
      "[0.105911] [0.06172613] 0.0018231582004783803\n",
      "[0.11025994] [0.06428095] 0.0017820589359922871\n",
      "[0.11455186] [0.0668042] 0.0017420145286063963\n",
      "[0.11878751] [0.06929629] 0.0017029978867139147\n",
      "[0.12296761] [0.07175761] 0.0016649826145092724\n",
      "[0.12709289] [0.07418856] 0.001627942994117653\n",
      "[0.13116404] [0.07658954] 0.0015918539681834994\n",
      "[0.13518178] [0.07896093] 0.0015566911229062013\n",
      "[0.13914679] [0.08130312] 0.0015224306715114885\n",
      "[0.14305975] [0.08361647] 0.0014890494381473223\n",
      "[0.14692133] [0.08590137] 0.0014565248421934067\n",
      "[0.15073221] [0.08815818] 0.001424834882973672\n",
      "[0.15449304] [0.09038727] 0.0013939581248613982\n",
      "[0.15820446] [0.09258898] 0.0013638736827668867\n",
      "[0.16186711] [0.09476367] 0.0013345612079978546\n",
      "[0.16548163] [0.0969117] 0.001306000874482983\n",
      "[0.16904863] [0.0990334] 0.0012781733653492889\n",
      "[0.17256873] [0.10112912] 0.0012510598598442348\n",
      "[0.17604254] [0.10319919] 0.0012246420205937207\n",
      "[0.17947065] [0.10524395] 0.0011989019811873343\n",
      "[0.18285367] [0.10726371] 0.0011738223340824504\n",
      "[0.18619216] [0.1092588] 0.001149386118818998\n",
      "[0.1894867] [0.11122954] 0.0011255768105369046\n",
      "[0.19273788] [0.11317625] 0.0011023783087884525\n",
      "[0.19594623] [0.11509922] 0.0010797749266379757\n",
      "[0.19911233] [0.11699878] 0.0010577513800415048\n",
      "[0.20223671] [0.11887522] 0.0010362927774991823\n",
      "[0.20531991] [0.12072884] 0.0010153846099734368\n",
      "[0.20836247] [0.12255993] 0.0009950127410660909\n",
      "[0.21136492] [0.12436879] 0.0009751633974477556\n",
      "[0.21432777] [0.1261557] 0.0009558231595330266\n",
      "[0.21725153] [0.12792094] 0.0009369789523951749\n",
      "[0.22013671] [0.1296648] 0.000918618036914175\n",
      "[0.22298381] [0.13138755] 0.0009007280011520846\n",
      "[0.22579332] [0.13308945] 0.0008832967519499293\n",
      "[0.22856573] [0.13477079] 0.0008663125067404105\n",
      "[0.23130151] [0.13643183] 0.0008497637855708867\n",
      "[0.23400115] [0.13807282] 0.0008336394033312334\n",
      "[0.2366651] [0.13969403] 0.0008179284621813159\n",
      "[0.23929383] [0.14129571] 0.0008026203441729488\n",
      "[0.2418878] [0.14287812] 0.0007877047040613505\n",
      "[0.24444746] [0.14444149] 0.0007731714623012211\n",
      "[0.24697324] [0.14598609] 0.0007590107982227049\n",
      "[0.24946559] [0.14751214] 0.0007452131433826147\n",
      "[0.25192494] [0.14901989] 0.0007317691750864224\n",
      "[0.25435172] [0.15050958] 0.0007186698100766238\n",
      "[0.25674634] [0.15198143] 0.000705906198383206\n",
      "[0.25910923] [0.15343568] 0.000693469717332058\n",
      "[0.26144079] [0.15487255] 0.0006813519657072616\n",
      "[0.26374143] [0.15629227] 0.0006695447580633175\n",
      "[0.26601155] [0.15769506] 0.0006580401191834495\n",
      "[0.26825155] [0.15908114] 0.000646830278680238\n",
      "[0.27046181] [0.16045072] 0.0006359076657349293\n",
      "[0.27264272] [0.16180401] 0.0006252649039718535\n",
      "[0.27479465] [0.16314123] 0.0006148948064644867\n",
      "[0.27691799] [0.16446259] 0.0006047903708697728\n",
      "[0.27901311] [0.16576828] 0.0005949447746874121\n",
      "[0.28108036] [0.1670585] 0.0005853513706409078\n",
      "[0.28312011] [0.16833347] 0.0005760036821772412\n",
      "[0.28513271] [0.16959337] 0.0005668953990821303\n",
      "[0.28711852] [0.1708384] 0.0005580203732079046\n",
      "[0.28907788] [0.17206875] 0.0005493726143110996\n",
      "[0.29101114] [0.17328461] 0.0005409462859969583\n",
      "[0.29291863] [0.17448616] 0.0005327357017680881\n",
      "[0.29480068] [0.1756736] 0.0005247353211746044\n",
      "[0.29665763] [0.1768471] 0.0005169397460631493\n",
      "[0.2984898] [0.17800684] 0.0005093437169222482\n",
      "[0.30029751] [0.179153] 0.0005019421093215306\n",
      "[0.30208108] [0.18028575] 0.0004947299304424013\n",
      "[0.30384081] [0.18140527] 0.00048770231569781746\n",
      "[0.30557702] [0.18251173] 0.00048085452543888084\n",
      "[0.30729002] [0.18360529] 0.0004741819417460142\n",
      "[0.30898009] [0.18468613] 0.0004676800653025513\n",
      "[0.31064755] [0.1857544] 0.0004613445123486251\n",
      "[0.31229268] [0.18681026] 0.0004551710117132875\n",
      "[0.31391576] [0.18785389] 0.00044915540192285586\n",
      "[0.31551709] [0.18888542] 0.00044329362838352635\n",
      "[0.31709695] [0.18990503] 0.0004375817406363464\n",
      "[0.31865562] [0.19091287] 0.0004320158896826898\n",
      "[0.32019336] [0.19190907] 0.00042659232537842033\n",
      "[0.32171045] [0.19289381] 0.00042130739389498433\n",
      "[0.32320717] [0.19386721] 0.0004161575352457093\n",
      "[0.32468376] [0.19482944] 0.00041113928087563534\n",
      "[0.32614049] [0.19578062] 0.00040624925131324763\n",
      "[0.32757762] [0.19672091] 0.0004014841538825212\n",
      "[0.3289954] [0.19765045] 0.0003968407804737269\n",
      "[0.33039409] [0.19856937] 0.0003923160053714899\n",
      "[0.33177393] [0.19947781] 0.00038790678313863297\n",
      "[0.33313515] [0.2003759] 0.0003836101465543672\n",
      "[0.33447802] [0.20126379] 0.0003794232046054379\n",
      "[0.33580275] [0.20214159] 0.00037534314052886517\n",
      "[0.3371096] [0.20300944] 0.00037136720990495144\n",
      "[0.33839878] [0.20386746] 0.0003674927387992692\n",
      "[0.33967052] [0.20471579] 0.00036371712195236655\n",
      "[0.34092506] [0.20555455] 0.0003600378210159681\n",
      "[0.34216262] [0.20638386] 0.0003564523628344766\n",
      "[0.34338341] [0.20720385] 0.00035295833777060934\n",
      "[0.34458765] [0.20801462] 0.00034955339807404005\n",
      "[0.34577556] [0.20881631] 0.0003462352562919374\n",
      "[0.34694735] [0.20960902] 0.00034300168372032813\n",
      "[0.34810322] [0.21039288] 0.00033985050889523183\n",
      "[0.34924339] [0.211168] 0.00033677961612255147\n",
      "[0.35036805] [0.21193448] 0.00033378694404571723\n",
      "[0.35147741] [0.21269245] 0.00033087048425012224\n",
      "[0.35257166] [0.213442] 0.0003280282799033975\n",
      "[0.353651] [0.21418326] 0.0003252584244306114\n",
      "[0.35471563] [0.21491632] 0.0003225590602234928\n",
      "[0.35576573] [0.21564129] 0.0003199283773828065\n",
      "[0.35680149] [0.21635827] 0.0003173646124930288\n",
      "[0.3578231] [0.21706737] 0.00031486604742849225\n",
      "[0.35883074] [0.21776869] 0.0003124310081901938\n",
      "[0.3598246] [0.21846233] 0.00031005786377247713\n",
      "[0.36080484] [0.21914839] 0.00030774502505882166\n",
      "[0.36177165] [0.21982695] 0.00030549094374599276\n",
      "[0.3627252] [0.22049813] 0.00030329411129582186\n",
      "[0.36366566] [0.22116202] 0.0003011530579139089\n",
      "[0.36459321] [0.2218187] 0.00029906635155455544\n",
      "[0.365508] [0.22246828] 0.0002970325969512539\n",
      "[0.36641021] [0.22311084] 0.00029505043467207713\n",
      "[0.36729999] [0.22374647] 0.00029311854019932834\n",
      "[0.36817751] [0.22437527] 0.0002912356230328295\n",
      "[0.36904293] [0.22499731] 0.00028940042581623984\n",
      "[0.3698964] [0.22561269] 0.0002876117234858127\n",
      "[0.37073808] [0.22622149] 0.0002858683224410161\n",
      "[0.37156812] [0.2268238] 0.00028416905973645414\n",
      "[0.37238668] [0.2274197] 0.0002825128022945425\n",
      "[0.37319389] [0.22800927] 0.0002808984461384044\n",
      "[0.37398991] [0.22859259] 0.00027932491564447003\n",
      "[0.37477489] [0.22916975] 0.0002777911628142697\n",
      "[0.37554896] [0.22974082] 0.0002762961665649307\n",
      "[0.37631226] [0.23030588] 0.0002748389320378973\n",
      "[0.37706494] [0.23086501] 0.0002734184899254031\n",
      "[0.37780714] [0.23141828] 0.0002720338958142438\n",
      "[0.37853899] [0.23196578] 0.0002706842295464026\n",
      "[0.37926062] [0.23250756] 0.0002693685945960982\n",
      "[0.37997216] [0.23304371] 0.00026808611746283087\n",
      "[0.38067375] [0.2335743] 0.00026683594708001797\n",
      "[0.38136552] [0.2340994] 0.00026561725423881723\n",
      "[0.3820476] [0.23461908] 0.00026442923102674884\n",
      "[0.3827201] [0.23513341] 0.0002632710902807343\n",
      "[0.38338315] [0.23564246] 0.00026214206505418473\n",
      "[0.38403688] [0.23614629] 0.00026104140809777443\n",
      "[0.3846814] [0.23664497] 0.0002599683913535508\n",
      "[0.38531684] [0.23713858] 0.0002589223054620375\n",
      "[0.38594332] [0.23762717] 0.0002579024592819957\n",
      "[0.38656094] [0.2381108] 0.00025690817942252043\n",
      "166\n"
     ]
    }
   ],
   "source": [
    "  def gd2(obsX, obsY, alpha, threshold):\n",
    "    \"\"\"\n",
    "    Input : observed vectors X, Y, alpha and threshold\n",
    "    Return theta0, theta1 from Gradient Descent L2 loss algorithm\n",
    "    Return: Iterations and L2 Error\n",
    "    \"\"\"\n",
    "    track = 0\n",
    "    t0 = 0\n",
    "    t1 = 0\n",
    "    \n",
    "    Ylen = len(obsY)\n",
    "    Xlen = len(obsX)\n",
    "    \n",
    "    Pt0 = 0\n",
    "    Pt0 = 0\n",
    "   \n",
    "    x1 =((obsX * t1) + t0)\n",
    "    minus1 = x1 - obsY\n",
    "    \n",
    "    t0 = t0 - alpha * sum(minus1)/Ylen\n",
    "    t1 = t1 - alpha * sum(minus1 * obsX) /Xlen\n",
    "    track = 1\n",
    "    PE = sqerror(obsX,obsY,theta0,theta1)\n",
    "    constant = 1\n",
    "    NE = PE + threshold + constant\n",
    "    \n",
    "    \n",
    "    while(abs(NE - PE) > threshold):\n",
    "        x1 = obsX * t1 + t0\n",
    "        minus1 = x1 - obsY\n",
    "        \n",
    "        t0 = t0 - alpha * sum(minus1)/Ylen\n",
    "        t1 = t1 - alpha * sum(minus1 * obsX) /Xlen\n",
    "        track += 1\n",
    "        PE = NE\n",
    "        NE = np.mean((obsX * t1 + t0 - obsY) ** 2) /Ylen\n",
    "\n",
    "        print(t0,t1,NE)\n",
    "    return t0, t1, NE, track\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "[theta0,theta1,newError,iterations] = gd2(x,y,0.01,0.000001)\n",
    "print(iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.38656094]), array([0.2381108]))"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta0, theta1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Gradient Descent (Huber)\n",
    "Compute a formula for Pseudo huber gradient descent using derivative methods. Similar to L2 descent, use the new formulas (obtained from pseudo huber derivatives) to compute values of theta1, theta1, error. The pseudo huber loss function is provided in Part 2.3. Use that to differentiate the huber function to theta0 and theta1. \n",
    "\n",
    "Print out theta0 and theta1 for each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003258873696563515 1e-06\n",
      "0.004837509915910264 0.003258873696563515 1e-06\n",
      "0.004837509915910264 0.004837509915910264 0.004837509915910264\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# given the observed data (obsX,obsY), learning rate (alpha), and desired error, \n",
    "# the function returns theta0, theta1, error and iterations\n",
    "# that reaches a minimum error threshold\n",
    "\n",
    "\n",
    "\n",
    "def gdh(obsX, obsY, alpha, threshold, delta):\n",
    "    \"\"\"\n",
    "    Input : observed vectors X, Y, alpha and threshold\n",
    "    Return theta0, theta1 from Gradient Descent huber loss algorithm\n",
    "    Return: Iterations and huber Error\n",
    "    \"\"\"\n",
    "    count = 1\n",
    "    PT0 = 0\n",
    "    PT1 = 0\n",
    "    \n",
    "    TC = PT0 - alpha*(np.sum(PT1*obsX+PT0)-obsY)\n",
    "    TP = PT1 - alpha*(np.sum(PT1*obsX+PT0-obsY)*obsX)\n",
    "    \n",
    "    PR = huberror(obsX,obsY,TC,TP,delta)\n",
    "    #First print\n",
    "    print(PR, threshold)\n",
    "    PT0 = TC\n",
    "    PT1 = TP\n",
    "    count+=1\n",
    "    \n",
    "    TC = PT0 - alpha*(np.sum(PT1*obsX+PT0)-obsY)\n",
    "    TP = PT1 - alpha*(np.sum(PT1*obsX+PT0-obsY)*obsX)\n",
    "    \n",
    "    NR =huberror(obsX,obsY,TC, TP,delta)\n",
    "    #Second Print\n",
    "    print(NR, PR, threshold)\n",
    "    while(abs(NR-PR) > threshold):\n",
    "        \n",
    "        PR = NR\n",
    "        PT0 = TC\n",
    "        PT1 = TP\n",
    "        count +=1\n",
    "        #Last print\n",
    "        print(NR, PR, PR)\n",
    "    return [TC,TP,NR,count]\n",
    "    print(TC, TP, NR)\n",
    "\n",
    "# testing    \n",
    "[theta0,theta1,newError,iterations] = gdh(x,y,0.01,0.000001,0.01)\n",
    "print(iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  What values provided the min value through gradient descent?\n",
    "1. The values of theta0, theta1, alpha, error that provided the minimum value through gradient descent:\n",
    "    Theta0: .004837509915910264, Theta1: 0.004837509915910264, and alpha: 0.004837509915910264\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with Library Estimators\n",
    "Using the sklearn LinearRegression module to automate this process find the differences in intercept value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.29620134]\n",
      "[[0.5238794]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()\n",
    "result = lm.fit(x,y)\n",
    "print(result.intercept_)\n",
    "print(result.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02342046127015977"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta0 = result.intercept_\n",
    "theta1 = result.coef_\n",
    "sqerror(x, y, theta0,theta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the Final Exam Score\n",
    "The regression line was obtained using grades from CS 205 course. We can consider them to be training data. Now we trained a model (with theta0 and theta1) so we can predict the grade for your own course based on your midterm grade.\n",
    "We will do few things before we can accomplish this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the supplied data into the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 97 entries, 0 to 96\n",
      "Data columns (total 1 columns):\n",
      "midterm    97 non-null float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 856.0 bytes\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 96 entries, 0 to 96\n",
      "Data columns (total 1 columns):\n",
      "midterm    96 non-null float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 1.5 KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_439 = pd.read_csv(\"data/CS439_grades_03_15_19.csv\")\n",
    "df_439.info()\n",
    "mid = df_439[df_439['midterm']<80]\n",
    "mid.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the Grade\n",
    "By scaling the values in the midterm grades of CS 439 and compute the estimated final exam grade. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.5 [[61.9]]\n",
      "42.3 [[51.9]]\n",
      "52.6 [[57.2]]\n",
      "12.8 [[36.5]]\n",
      "94.9 [[79.3]]\n",
      "56.4 [[59.2]]\n",
      "39.7 [[50.5]]\n",
      "46.2 [[53.9]]\n",
      "43.6 [[52.6]]\n",
      "21.8 [[41.2]]\n",
      "48.7 [[55.2]]\n",
      "43.6 [[52.6]]\n",
      "75.6 [[69.3]]\n",
      "65.4 [[63.9]]\n",
      "52.6 [[57.2]]\n",
      "43.6 [[52.6]]\n",
      "61.5 [[61.9]]\n",
      "42.3 [[51.9]]\n",
      "60.3 [[61.3]]\n",
      "100.0 [[82.]]\n",
      "70.5 [[66.6]]\n",
      "61.5 [[61.9]]\n",
      "0.0 [[29.8]]\n",
      "53.8 [[57.9]]\n",
      "34.6 [[47.9]]\n",
      "83.3 [[73.3]]\n",
      "67.9 [[65.3]]\n",
      "91.0 [[77.3]]\n",
      "29.5 [[45.2]]\n",
      "73.1 [[67.9]]\n",
      "67.9 [[65.3]]\n",
      "2.6 [[31.1]]\n",
      "61.5 [[61.9]]\n",
      "69.2 [[65.9]]\n",
      "48.7 [[55.2]]\n",
      "98.7 [[81.3]]\n",
      "34.6 [[47.9]]\n",
      "60.3 [[61.3]]\n",
      "44.9 [[53.2]]\n",
      "50.0 [[55.9]]\n",
      "69.2 [[65.9]]\n",
      "60.3 [[61.3]]\n",
      "83.3 [[73.3]]\n",
      "26.9 [[43.9]]\n",
      "61.5 [[61.9]]\n",
      "20.5 [[40.5]]\n",
      "19.2 [[39.8]]\n",
      "6.4 [[33.1]]\n",
      "80.8 [[72.]]\n",
      "44.9 [[53.2]]\n",
      "26.9 [[43.9]]\n",
      "14.1 [[37.2]]\n",
      "67.9 [[65.3]]\n",
      "51.3 [[56.6]]\n",
      "24.4 [[42.5]]\n",
      "78.2 [[70.6]]\n",
      "23.1 [[41.8]]\n",
      "60.3 [[61.3]]\n",
      "71.8 [[67.3]]\n",
      "85.9 [[74.6]]\n",
      "50.0 [[55.9]]\n",
      "37.2 [[49.2]]\n",
      "73.1 [[67.9]]\n",
      "62.8 [[62.6]]\n",
      "59.0 [[60.6]]\n",
      "70.5 [[66.6]]\n",
      "5.1 [[32.5]]\n",
      "34.6 [[47.9]]\n",
      "19.2 [[39.8]]\n",
      "50.0 [[55.9]]\n",
      "67.9 [[65.3]]\n",
      "7.7 [[33.8]]\n",
      "67.9 [[65.3]]\n",
      "67.9 [[65.3]]\n",
      "53.8 [[57.9]]\n",
      "39.7 [[50.5]]\n",
      "43.6 [[52.6]]\n",
      "98.7 [[81.3]]\n",
      "79.5 [[71.3]]\n",
      "88.5 [[76.]]\n",
      "55.1 [[58.6]]\n",
      "65.4 [[63.9]]\n",
      "57.7 [[59.9]]\n",
      "46.2 [[53.9]]\n",
      "51.9 [[56.9]]\n",
      "70.5 [[66.6]]\n",
      "51.3 [[56.6]]\n",
      "43.6 [[52.6]]\n",
      "6.4 [[33.1]]\n",
      "30.8 [[45.9]]\n",
      "96.2 [[80.]]\n",
      "69.2 [[65.9]]\n",
      "64.1 [[63.3]]\n",
      "38.5 [[49.9]]\n",
      "42.3 [[51.9]]\n",
      "83.3 [[73.3]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "sep = 'midterm'\n",
    "extend =  preprocessing.MinMaxScaler(feature_range=(29.8,82))\n",
    "\n",
    "\n",
    "ms = Full_List.fit_transform(mid[[sep]])\n",
    "\n",
    "\n",
    "fs = extend.fit_transform(ms)\n",
    "ms= np.round(ms,1)\n",
    "ms= ms.tolist()\n",
    "\n",
    "ms=[x[0] \n",
    "      for x in ms]\n",
    "#fs=[x[0]\n",
    "      # for x in fs]\n",
    "    \n",
    "fs = np.round(fs,1)\n",
    "Flen = len(final_score)\n",
    "\n",
    "for x in range(0,Flen):\n",
    "    print(ms[x], fs[[x]])\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
